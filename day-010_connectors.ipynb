{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10 - How to Connect to Different Data Sources and Sinks\n",
    "So far, I've connected my queries to two different data source types, CSV and JSON, but there are mony more types of datasorces or sinks Spark can read/write data from/to, like Paquet, files, XML files or JDBC/ODBC database connections, or Hive Tables. \n",
    "\n",
    "My task for today is to investigate the generic structure of Spark connectors which are implemented by the `pyspark.sql.readwriter` module. Since reading and writing data are tasks in every Spark application, maybe I can derive some basic pattern which I can use for re-usablecode templates.\n",
    "\n",
    "## Class pyspark.sql.readwriter.<a href=\"https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader\">DataFrameReader<a/>\n",
    "\n",
    "Reading data is a data source -> DataFrame transformation. Since there is no input DataFrame for this transformation I cannot apply it on a DataFrame. Instead the data read API is bound to the `SparkSession`, the access path is:\n",
    "\n",
    "`SparkSession.read`\n",
    "\n",
    "As I observered several times before, there are again multiple options in Spark to get the same results. Actually there are three differnt was of how to read or write data to/from external resources.\n",
    "    \n",
    "The first layout version for data reading puts every option in a single function call. Some options have even its own function name like `format()` and `schema()`. All these option functions are `DataFrameReader` -> `DataFrameReader` transformations so they can be sticked together. Only the `load()` function must be the last on in the chain, because it is a `DataFrameReader` -> `DataFrame` transformation. This is the layout version I've used so far, because I was not aware of the other versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()\n",
    "\n",
    "myOwnCsv = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",StringType(),False)\n",
    "])\n",
    "\n",
    "csvDF = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"path\", \"./data/flight-data/2015-summary.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(myOwnCsv)\\\n",
    "    .load()\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layout passes all options as arguments into the `load()` function. This is the most generic layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF = spark.read\\\n",
    "    .load(\n",
    "        format=\"csv\",\n",
    "        path=\"./data/flight-data/2015-summary.csv\",\n",
    "        schema=myOwnCsv,\n",
    "        header=True\n",
    "    )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third layout uses format-specific load functions, e.g. `csv()` to load CSV files. This is the most concise layout and my favourite one from now on. \n",
    "\n",
    "pyspark provides the following load functions out-of the box\n",
    "* `csv()`\n",
    "* `jdbc()`\n",
    "* `json()`\n",
    "* `paquet()`\n",
    "* `orc()`\n",
    "* `table()`\n",
    "* `text()`\n",
    "\n",
    "By the way: *Paquet* is the default format in Spark, because it is column-orientated, which supports column compression and splittable. So if I don't specify the `format` option, Spark or pyspark will take the Parquet format for both read and write operations.\n",
    "\n",
    "Each data source format has its own subset of available options, so I have to reference the pyspark documentation to check, which options are applicable, optional or mandatory. Nonetheless, one generic pattern is that for all file based formats I must define the `path` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF = spark.read\\\n",
    "    .csv(\n",
    "        path=\"./data/flight-data/2015-summary.csv\",\n",
    "        header=True,\n",
    "        schema=myOwnCsv\n",
    "    )\n",
    "\n",
    "csvDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class pyspark.sql.readwriter.<a href=\"https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameWriter\">DataFrameWriter<a/>\n",
    "\n",
    "Writing data is a DataFrame -> data sink transformation. Therfore the data write API is bound to the `DataFrame`, the access path is:\n",
    "\n",
    "`DataFrame.write`\n",
    "\n",
    "The layout variants are nearly the same as for the reading API except for `load()` is replaced by save functions. \n",
    "pyspark provides the following format specific save functions out-of the box:\n",
    "* `csv()`\n",
    "* `jdbc()`\n",
    "* `json()`\n",
    "* `paquet()`\n",
    "* `orc()`\n",
    "* `saveAsTable()`or `ìnsertInto()`\n",
    "* `text()`\n",
    "    \n",
    "A further generic write parameter is the *save mode* which specifies the behavior of the save operation when data already exists.\n",
    "* `append`: Append contents of this DataFrame to existing data.\n",
    "* `overwrite`: Overwrite existing data.\n",
    "* `ignore`: Silently ignore this operation if data already exists.\n",
    "* `error` or `errorifexists` (default case): Throw an exception if data already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.write.csv(\n",
    "    path=\"./data/flight-data/2015-output.csv\",\n",
    "    header=True,\n",
    "    sep=\";\",\n",
    "    mode=\"overwrite\",\n",
    "    encoding=\"utf-8\",\n",
    "    compression=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will create a folder \"2015-output\" which represents the DataFrame and store one csv file for each partition into it using UTF-8 encoding. Values are separated by semicolons and existing files will be overwritten.\n",
    "## Reading and Writing JSON Files\n",
    "There is an aspect regarding JSON files, I should keep in mind. By default, pyspark assumes that a source JSON file is actually <a href=\"http://jsonlines.org/\">newline-delimited JSON</a>, i.e. the file contains only single line JSON objects but many of them.\n",
    "\n",
    "Example:\n",
    "\n",
    "`{\"ORIGIN_COUNTRY_NAME\":\"Romania\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":15}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"Croatia\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":1}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"Ireland\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":344}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Egypt\",\"count\":15}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"India\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":62}`\n",
    "\n",
    "These simplified version of JSON is optimal for record by record processing. Neveretheless the original JSON format allows files containing one big complex JSON object or an array of nested objects with multiple hierarchy levels.\n",
    "\n",
    "Example:\n",
    "\n",
    "`{\"customers\":[\n",
    "    {\n",
    "        \"cusomerId\":1,\n",
    "        \"address:{\"street\":\"Reeperbahn 2\". \"city\":\"Hamburg\", \"country\":\"Germany\"},\n",
    "        \"date-of-birth\":\"1980-12-17\",\n",
    "        \"names\":{\"currentName\":\"Mayer\", \"givenName\":\"Schmitz\"}\n",
    "    },\n",
    "    {\n",
    "        \"cusomerId\":2,\n",
    "        \"address:{\"street\":\"Aachener Strasse 234\". \"city\":\"Cologne\", \"country\":\"Germany\"},\n",
    "        \"date-of-birth\":\"1978-06-27\",\n",
    "        \"names\":{\"currentName\":\"Müller\", \"givenName\":\"\"}\n",
    "    },\n",
    "    ... \n",
    "    ]\n",
    "}`\n",
    "\n",
    "To read those complex JSON files, I need to define the option `multiLine=True` (default is False)\n",
    "## Reading and Writing Paquet Files\n",
    "Paquet files include their own schema definition and enforcement, so when I write data to a Paquet file, I won't have any schema option. Reading from Paquet file always implies schema inference. The only schema related option I have here is to set `mergeSchema=True` to merge schemas collected from Parquet part-files having divergent schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
