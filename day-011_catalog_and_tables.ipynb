{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11 - Managing Metadata: Catalog, Tables and Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API\n",
    "###  Class: pyspark.sql.session.<a href=\"https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.SparkSession\">SparkSession</a>\n",
    "Usually the SparkSession object is usually assigned to the variable named *spark*. \n",
    "\n",
    "#### Object Properties:\n",
    "* **catalog** - to access the the `Catalog`interface for maintaining metadata regarding databases, tables, functions, etc.\n",
    "\n",
    "#### Object Methods: (reading from data source Table -> DataFrame)\n",
    "* **table()** - Returns the specified table as a DataFrame\n",
    "\n",
    "This function corresponds to the `DataFrameReader`, I'm using to read from file based data sources.\n",
    "\n",
    "### Class: pyspark.sql.dataframe.<a href=\"https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">DataFrame</a>\n",
    "\n",
    "#### Object Properties:\n",
    "* **write** - to access the `DataFrameWriter` interface for writing from a DataFrame to a data sink\n",
    "* **writeStream** - to access the `DataStreamWriter` object for writing Stream data to external storage.\n",
    "\n",
    "#### Object Methods:\n",
    "* **createGlobalTempView()**\n",
    "* **createOrReplaceGlobalTempView()** \n",
    "* **createOrReplaceTempView()** \n",
    "* **createTempView()** \n",
    "* **registerTempTable()** \n",
    "\n",
    "Tha function names indicate the following characteristics:\n",
    "* *Temp*, i.e. temporary, means the lifetime of the table/view is tied to the SparkSession that was used to create it\n",
    "* *Global* means, the tabls/view is known to all clusters, all others are locally to a cluster\n",
    "* *orReplace* means, no exception is thrown, if a table/view already exists in the Catalog.\n",
    "\n",
    "### Class: pyspark.sql.readwriter.<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter\">DataFrameWriter</a>\n",
    "\n",
    "Interface used to write a `DataFrame` to external storage systems (e.g. file systems, key-value stores, etc). Accessed through the `DataFrame.write` property\n",
    "#### Object Methods: (writing DataFrame -> Table as data sink)\n",
    "* **bucketBy()** - Buckets the output by the given columns.If specified, the output is laid out on the file system similar to Hiveâ€™s bucketing scheme\n",
    "* **insertInto()** - Inserts the content of the DataFrame to the specified table.It requires that the schema of the `DataFrame` is the same as the schema of the table.\n",
    "* **saveAsTable()** - Saves the content of the DataFrame as the specified table.\n",
    "\n",
    "### Class: pyspark.sql.catalog.<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Catalog\">Catalog</a> [abstract]\n",
    "Interface to the `Catalog`, accessed through the `SparkSession.catalog`property.\n",
    "\n",
    "To me, it looks like this *pyspark* interface class provides only a sub-set of the Spark Catalog functionality. To have all meta-data functions available, I must use the <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html\">SQL API</a>. Therefore I list just the most important functions, just to have a reminder, how pyspark inter-acts with the Spark Catalog.\n",
    "#### Class Functions:\n",
    "* **createExternalTable()** - Creates an **unmanaged** table based on the dataset in a data source. It returns the DataFrame associated with the external table. \n",
    "* **createTable()** - Creates a table based on the dataset in a data source. It returns the DataFrame associated with the table. When path is specified, an **unmanaged** external table is created from the data at the given path. Otherwise a **managed** table is created.Optionally.\n",
    "* **dropGlobalTempView()** - Drops the global temporary view with the given view name in the catalog. \n",
    "* **dropTempView()** - Drops the local temporary view with the given view name in the catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL API\n",
    "Ref. <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html\">SQL Language Reference</a> provided by Databricks.\n",
    "\n",
    "Only the SQL API provides the full scope of metadata management functionality in the Spark Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
