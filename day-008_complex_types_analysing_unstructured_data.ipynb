{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8 - Using Complex Types to Analyse Unstructured or JSON Data\n",
    "My challenge of today is to go beyond processing well structured data, which complies to a schema and where all values are clearly seperated into typed columns. Today I want to analyse the stock descriptions in the retail data set, which come as unstructured text. This is my use case to investigate Sparks complex datataypes like arrays and maps. Next to that, I want to get familiar with the processing of semi-structured data like JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()\n",
    "\n",
    "retailDF = spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .load(\"./data/day-008/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two questions, I want to investigate regarding the description data:\n",
    "1. What is the average number of words in the Description per StockCode?\n",
    "1. Which are the most frequently used words?\n",
    "\n",
    "## Data Preparation\n",
    "The granularity of my analysis is StockCode and not individual invoice items. So to prevent StockCode duplicates, I tailor the data set to get a DataFrame containing distinct StockCodes and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+\n",
      "|StockCode|Description                |\n",
      "+---------+---------------------------+\n",
      "|10002    |null                       |\n",
      "|10002    |INFLATABLE POLITICAL GLOBE |\n",
      "|10080    |null                       |\n",
      "|10080    |GROOVY CACTUS INFLATABLE   |\n",
      "|10080    |check                      |\n",
      "|10120    |DOGGY RUBBER               |\n",
      "|10123C   |null                       |\n",
      "|10123C   |HEARTS WRAPPING TAPE       |\n",
      "|10123G   |null                       |\n",
      "|10124A   |SPOTS ON RED BOOKCOVER TAPE|\n",
      "+---------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinctDF = retailDF.select(\n",
    "        \"StockCode\",\n",
    "        \"Description\").distinct()\n",
    "\n",
    "distinctDF.orderBy(\"StockCode\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the null value problem, I investigated yesterday, occures again. Rows having null values in any column are uselesss for my analysis, so I want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+\n",
      "|StockCode|Description                 |\n",
      "+---------+----------------------------+\n",
      "|10002    |INFLATABLE POLITICAL GLOBE  |\n",
      "|10080    |check                       |\n",
      "|10080    |GROOVY CACTUS INFLATABLE    |\n",
      "|10120    |DOGGY RUBBER                |\n",
      "|10123C   |HEARTS WRAPPING TAPE        |\n",
      "|10124A   |SPOTS ON RED BOOKCOVER TAPE |\n",
      "|10124G   |ARMY CAMO BOOKCOVER TAPE    |\n",
      "|10125    |MINI FUNKY DESIGN TAPES     |\n",
      "|10133    |damaged                     |\n",
      "|10133    |COLOURING PENCILS BROWN TUBE|\n",
      "+---------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanedDF = distinctDF.dropna(how=\"any\")\n",
    "\n",
    "cleanedDF.orderBy(\"StockCode\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays\n",
    "Next I've to do is to split up the text strings into arrays of words. The words in the descriptions are seperated by blanks, so I define this as spit seperator. The result looks like Python lists but in contrast to lists, all array elements must have the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+\n",
      "|StockCode|word_list                                 |\n",
      "+---------+------------------------------------------+\n",
      "|21285    |[RETROSPOT, CANDLE, , MEDIUM]             |\n",
      "|84987    |[SET, OF, 36, TEATIME, PAPER, DOILIES]    |\n",
      "|22708    |[WRAP, DOLLY, GIRL]                       |\n",
      "|22690    |[DOORMAT, HOME, SWEET, HOME, BLUE, ]      |\n",
      "|21249    |[WOODLAND, , HEIGHT, CHART, STICKERS, ]   |\n",
      "|85015    |[SET, OF, 12, , VINTAGE, POSTCARD, SET]   |\n",
      "|84279P   |[CHERRY, BLOSSOM, , DECORATIVE, FLASK]    |\n",
      "|23623    |[SET, 10, CARD, CHRISTMAS, WELCOME, 17112]|\n",
      "|23432    |[PRETTY, HANGING, QUILTED, HEARTS]        |\n",
      "|21002    |[ROSE, DU, SUD, DRAWSTRING, BAG]          |\n",
      "+---------+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "splittedDF = cleanedDF.select(\n",
    "        \"StockCode\",\n",
    "        split(\"Description\", \" \").alias(\"word_list\")\n",
    ")\n",
    "\n",
    "splittedDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with normal Python lists I can grab specific elements, i.e. words from my word lists, by referencing their index starting with 0 for the first element. So to get the second word in each description, I need to refer to index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|StockCode|word_list[1]|\n",
      "+---------+------------+\n",
      "|    21285|      CANDLE|\n",
      "|    84987|          OF|\n",
      "|    22708|       DOLLY|\n",
      "|    22690|        HOME|\n",
      "|    21249|            |\n",
      "|    85015|          OF|\n",
      "|   84279P|     BLOSSOM|\n",
      "|    23623|          10|\n",
      "|    23432|     HANGING|\n",
      "|    21002|          DU|\n",
      "+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "splittedDF.select(\"StockCode\", col(\"word_list\")[1]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that InvoiceNo 21249 seems to have a double blank after the first word. Maybe a typo in a free-text field? Anyway, I dont to count words, not blanks, so I have to removing them later. First, I want to double check, if this is a more general or single-case issue. \n",
    "\n",
    "I can easily check wether or not a word list contains specific key words by using the `array_contains()` function. For my analysis, I want to identify rows having empty words in the list, which I'dont want to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+--------------------+\n",
      "|StockCode|word_list                                 |empty strings inside|\n",
      "+---------+------------------------------------------+--------------------+\n",
      "|21285    |[RETROSPOT, CANDLE, , MEDIUM]             |true                |\n",
      "|84987    |[SET, OF, 36, TEATIME, PAPER, DOILIES]    |false               |\n",
      "|22708    |[WRAP, DOLLY, GIRL]                       |false               |\n",
      "|22690    |[DOORMAT, HOME, SWEET, HOME, BLUE, ]      |true                |\n",
      "|21249    |[WOODLAND, , HEIGHT, CHART, STICKERS, ]   |true                |\n",
      "|85015    |[SET, OF, 12, , VINTAGE, POSTCARD, SET]   |true                |\n",
      "|84279P   |[CHERRY, BLOSSOM, , DECORATIVE, FLASK]    |true                |\n",
      "|23623    |[SET, 10, CARD, CHRISTMAS, WELCOME, 17112]|false               |\n",
      "|23432    |[PRETTY, HANGING, QUILTED, HEARTS]        |false               |\n",
      "|21002    |[ROSE, DU, SUD, DRAWSTRING, BAG]          |false               |\n",
      "+---------+------------------------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "splittedDF.select(\n",
    "    \"StockCode\", \n",
    "    \"word_list\", \n",
    "    array_contains(\"word_list\", \"\").alias(\"empty strings inside\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets let's clean up the word lists and remove any empty words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+\n",
      "|StockCode|word_list                                 |\n",
      "+---------+------------------------------------------+\n",
      "|21285    |[RETROSPOT, CANDLE, MEDIUM]               |\n",
      "|84987    |[SET, OF, 36, TEATIME, PAPER, DOILIES]    |\n",
      "|22708    |[WRAP, DOLLY, GIRL]                       |\n",
      "|22690    |[DOORMAT, HOME, SWEET, HOME, BLUE]        |\n",
      "|21249    |[WOODLAND, HEIGHT, CHART, STICKERS]       |\n",
      "|85015    |[SET, OF, 12, VINTAGE, POSTCARD, SET]     |\n",
      "|84279P   |[CHERRY, BLOSSOM, DECORATIVE, FLASK]      |\n",
      "|23623    |[SET, 10, CARD, CHRISTMAS, WELCOME, 17112]|\n",
      "|23432    |[PRETTY, HANGING, QUILTED, HEARTS]        |\n",
      "|21002    |[ROSE, DU, SUD, DRAWSTRING, BAG]          |\n",
      "+---------+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_remove\n",
    "\n",
    "cleanedWordListDF = splittedDF.select(\n",
    "    \"StockCode\", \n",
    "    array_remove(\"word_list\", \"\").alias(\"word_list\")\n",
    ")\n",
    "\n",
    "cleanedWordListDF.show(10, truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+--------------------+\n",
      "|StockCode|word_list                                 |empty strings inside|\n",
      "+---------+------------------------------------------+--------------------+\n",
      "|21285    |[RETROSPOT, CANDLE, MEDIUM]               |false               |\n",
      "|84987    |[SET, OF, 36, TEATIME, PAPER, DOILIES]    |false               |\n",
      "|22708    |[WRAP, DOLLY, GIRL]                       |false               |\n",
      "|22690    |[DOORMAT, HOME, SWEET, HOME, BLUE]        |false               |\n",
      "|21249    |[WOODLAND, HEIGHT, CHART, STICKERS]       |false               |\n",
      "|85015    |[SET, OF, 12, VINTAGE, POSTCARD, SET]     |false               |\n",
      "|84279P   |[CHERRY, BLOSSOM, DECORATIVE, FLASK]      |false               |\n",
      "|23623    |[SET, 10, CARD, CHRISTMAS, WELCOME, 17112]|false               |\n",
      "|23432    |[PRETTY, HANGING, QUILTED, HEARTS]        |false               |\n",
      "|21002    |[ROSE, DU, SUD, DRAWSTRING, BAG]          |false               |\n",
      "+---------+------------------------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanedWordListDF.select(\n",
    "    \"StockCode\", \n",
    "    \"word_list\", \n",
    "    array_contains(\"word_list\", \"\").alias(\"empty strings inside\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes, it did.\n",
    "\n",
    "Back to my questions. Now, after having cleaned up the data the number of words per stock description is simply the array length which is provided by the `size()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|StockCode|num_of_words|\n",
      "+---------+------------+\n",
      "|    21285|           3|\n",
      "|    84987|           6|\n",
      "|    22708|           3|\n",
      "|    22690|           5|\n",
      "|    21249|           4|\n",
      "|    85015|           6|\n",
      "|   84279P|           4|\n",
      "|    23623|           6|\n",
      "|    23432|           4|\n",
      "|    21002|           5|\n",
      "+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "cleanedWordListDF.select(\n",
    "    \"StockCode\", \n",
    "    size(\"word_list\").alias(\"num_of_words\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "| avg_num_of_words|\n",
      "+-----------------+\n",
      "|4.031510851419032|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avgDF = cleanedWordListDF.select(\n",
    "    avg(\n",
    "        size(\"word_list\")\n",
    "    ).alias(\"avg_num_of_words\")\n",
    ")\n",
    "\n",
    "avgDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the answer to my first question is that stock descriptions are quite short, just about four words in average.\n",
    "\n",
    "Pyspark module <a href=https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions>pyspark.sql.functions</a> provides further array related functions, which I just list here for later reference:\n",
    "\n",
    "* **array()** - creates a new array column from a list of columns or column expressions that have the **same data type**\n",
    "* **array_distinct(col)** - Collection function: removes duplicate values from the array \n",
    "* **array_except(col1, col2)** - Collection function: returns an array of the elements in col1 but not in col2, without duplicates\n",
    "* **array_intersect(col1, col2)** - Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates \n",
    "* **array_join()** \n",
    "* **array_max()** - Collection function: returns the maximum value of the array\n",
    "* **array_min()** - Collection function: returns the maximum value of the array\n",
    "* **array_position()** - Collection function: Locates the position of the first occurrence of the given value in the given array\n",
    "* **array_repeat(col, count)** - Collection function: creates an array containing a column repeated count times\n",
    "* **array_sort(col)** - Collection function: sorts the input array in ascending order\n",
    "* **array_union(col1, col2)** - Collection function: returns an array of the elements in the union of col1 and col2, without duplicates\n",
    "* **arrays_overlap(a1, a2)** - Collection function: returns true if the arrays contain any common non-null element\n",
    "* **arrays_zip()** - Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays\n",
    "\n",
    "## Explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two answer my secondf question, it would be easier for me having all words in in column instead of spread across many lists. To turn array elements into rows, I need to apply the `explode()` function. As the name of the function indicates, this can heavily increase the number of rows and the values of all remaining columns get duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|StockCode|     words|\n",
      "+---------+----------+\n",
      "|    10002|INFLATABLE|\n",
      "|    10002|     GLOBE|\n",
      "|    10002| POLITICAL|\n",
      "|    10080|    CACTUS|\n",
      "|    10080|     check|\n",
      "|    10080|    GROOVY|\n",
      "|    10080|INFLATABLE|\n",
      "|    10120|    RUBBER|\n",
      "|    10120|     DOGGY|\n",
      "|   10123C|      TAPE|\n",
      "|   10123C|  WRAPPING|\n",
      "|   10123C|    HEARTS|\n",
      "|   10124A|        ON|\n",
      "|   10124A|      TAPE|\n",
      "|   10124A|     SPOTS|\n",
      "|   10124A| BOOKCOVER|\n",
      "|   10124A|       RED|\n",
      "|   10124G|      CAMO|\n",
      "|   10124G| BOOKCOVER|\n",
      "|   10124G|      ARMY|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explodedDF = cleanedWordListDF.select(\n",
    "    \"StockCode\",\n",
    "    explode(\"word_list\").alias(\"words\")\n",
    ")\n",
    "\n",
    "explodedDF.orderBy(\"StockCode\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anser to my second question is simply a count of rows per word sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|    words|word_count|\n",
      "+---------+----------+\n",
      "|      SET|       341|\n",
      "|     PINK|       317|\n",
      "|       OF|       255|\n",
      "|    HEART|       242|\n",
      "|  VINTAGE|       225|\n",
      "|     BLUE|       221|\n",
      "|      RED|       205|\n",
      "|      BAG|       172|\n",
      "|CHRISTMAS|       158|\n",
      "|    GLASS|       157|\n",
      "+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, count, lit\n",
    "\n",
    "explodedDF\\\n",
    "    .groupBy(\"words\")\\\n",
    "    .agg(count(lit(1)).alias(\"word_count\"))\\\n",
    "    .orderBy(desc(\"word_count\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pink stocks seems to be quite popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps\n",
    "For handling data in key:value structure, Spark provides another complex datatype: *maps*.\n",
    "\n",
    "My testdata does not provide key:value structured data. So first, I will transform my existing data into maps and second, I can investigate, how to handle key:value source data as an input to my ETL dataprocessing.\n",
    "\n",
    "### Creating Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------+\n",
      "|key                         |value                           |\n",
      "+----------------------------+--------------------------------+\n",
      "|[destination, origin, count]|[United States, Romania, 15]    |\n",
      "|[destination, origin, count]|[United States, Croatia, 1]     |\n",
      "|[destination, origin, count]|[United States, Ireland, 344]   |\n",
      "|[destination, origin, count]|[Egypt, United States, 15]      |\n",
      "|[destination, origin, count]|[United States, India, 62]      |\n",
      "|[destination, origin, count]|[United States, Singapore, 1]   |\n",
      "|[destination, origin, count]|[United States, Grenada, 62]    |\n",
      "|[destination, origin, count]|[Costa Rica, United States, 588]|\n",
      "|[destination, origin, count]|[Senegal, United States, 40]    |\n",
      "|[destination, origin, count]|[Moldova, United States, 1]     |\n",
      "+----------------------------+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlight = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(\"./data/day-008/flight-data/2015-summary.csv\")\n",
    "\n",
    "from pyspark.sql.functions import lit, struct, array\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "arrDF = dfFlight.select(\n",
    "    array(\n",
    "        lit(\"destination\"),\n",
    "        lit(\"origin\"),\n",
    "        lit(\"count\")\n",
    "    ).alias(\"key\"),\n",
    "    array(\n",
    "        \"DEST_COUNTRY_NAME\",\n",
    "        \"ORIGIN_COUNTRY_NAME\",\n",
    "        col(\"count\").cast(StringType())\n",
    "    ).alias(\"value\")\n",
    ")\n",
    "\n",
    "arrDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|data_map                                                          |\n",
      "+------------------------------------------------------------------+\n",
      "|[destination -> United States, origin -> Romania, count -> 15]    |\n",
      "|[destination -> United States, origin -> Croatia, count -> 1]     |\n",
      "|[destination -> United States, origin -> Ireland, count -> 344]   |\n",
      "|[destination -> Egypt, origin -> United States, count -> 15]      |\n",
      "|[destination -> United States, origin -> India, count -> 62]      |\n",
      "|[destination -> United States, origin -> Singapore, count -> 1]   |\n",
      "|[destination -> United States, origin -> Grenada, count -> 62]    |\n",
      "|[destination -> Costa Rica, origin -> United States, count -> 588]|\n",
      "|[destination -> Senegal, origin -> United States, count -> 40]    |\n",
      "|[destination -> Moldova, origin -> United States, count -> 1]     |\n",
      "+------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_from_arrays\n",
    "\n",
    "mapDF = arrDF.select(\n",
    "    map_from_arrays(\"key\", \"value\").alias(\"data_map\")\n",
    ")\n",
    "\n",
    "mapDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|data_map[destination]|\n",
      "+---------------------+\n",
      "|        United States|\n",
      "|        United States|\n",
      "|        United States|\n",
      "|                Egypt|\n",
      "|        United States|\n",
      "|        United States|\n",
      "|        United States|\n",
      "|           Costa Rica|\n",
      "|              Senegal|\n",
      "|              Moldova|\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.select(col(\"data_map\")[\"destination\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|data_map[origin]|\n",
      "+----------------+\n",
      "|         Romania|\n",
      "|         Croatia|\n",
      "|         Ireland|\n",
      "|   United States|\n",
      "|           India|\n",
      "|       Singapore|\n",
      "|         Grenada|\n",
      "|   United States|\n",
      "|   United States|\n",
      "|   United States|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.select(col(\"data_map\")[\"origin\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|map_keys(data_map)          |\n",
      "+----------------------------+\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "|[destination, origin, count]|\n",
      "+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "mapDF.select(\n",
    "        map_keys(\"data_map\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|map_values(data_map)            |\n",
      "+--------------------------------+\n",
      "|[United States, Romania, 15]    |\n",
      "|[United States, Croatia, 1]     |\n",
      "|[United States, Ireland, 344]   |\n",
      "|[Egypt, United States, 15]      |\n",
      "|[United States, India, 62]      |\n",
      "|[United States, Singapore, 1]   |\n",
      "|[United States, Grenada, 62]    |\n",
      "|[Costa Rica, United States, 588]|\n",
      "|[Senegal, United States, 40]    |\n",
      "|[Moldova, United States, 1]     |\n",
      "+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "mapDF.select(\n",
    "        map_values(\"data_map\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data I've processed so far looks at least semi-structured because the keys and values all appear in identical order. So there is still an implicit schema because all rows match to the same pattern:\n",
    "\n",
    "destination -> descVal, origin -> origValue, count -> cntVal\n",
    "\n",
    "What would happen, if rows have keys and values in different order? Because my testdata does not provide examples for this, I create a DataFrame manuall with synthetic data in multiple orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+\n",
      "|key                         |value                       |\n",
      "+----------------------------+----------------------------+\n",
      "|[destination, origin, count]|[United States, Germany, 10]|\n",
      "|[count, origin, destination]|[25, France, Spain]         |\n",
      "|[count, destination, origin]|[75, Italy, Spain]          |\n",
      "+----------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unstructuredDF = spark.createDataFrame(\n",
    "        [\n",
    "            ([\"destination\", \"origin\", \"count\"], [\"United States\", \"Germany\", \"10\"],), \n",
    "            ([\"count\", \"origin\", \"destination\"], [\"25\", \"France\", \"Spain\"],),\n",
    "            ([\"count\", \"destination\", \"origin\"], [\"75\", \"Italy\", \"Spain\"],)\n",
    "        ], \n",
    "        [\"key\", \"value\"]\n",
    ")\n",
    "\n",
    "unstructuredDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|data_map                                                      |\n",
      "+--------------------------------------------------------------+\n",
      "|[destination -> United States, origin -> Germany, count -> 10]|\n",
      "|[count -> 25, origin -> France, destination -> Spain]         |\n",
      "|[count -> 75, destination -> Italy, origin -> Spain]          |\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF2 = unstructuredDF.select(\n",
    "    map_from_arrays(\"key\", \"value\").alias(\"data_map\")\n",
    ")\n",
    "\n",
    "mapDF2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|data_map[origin]|\n",
      "+----------------+\n",
      "|         Germany|\n",
      "|          France|\n",
      "|           Spain|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF2.select(col(\"data_map\")[\"origin\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily the odering doesn't matter because I reference the values by keys and not by positions. Maps are more like dictionaries than lists or arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Maps into DataFrames\n",
    "\n",
    "So with my self-created map I can now investigate how to handle such data as input for my ETL process which finally will write data in tabular form into a file or database table. So as intermediate step, I will have to align more or less ordered *key:value* pairs with the schema of a `DataFrame`. \n",
    "\n",
    "Can the `explode()` function help again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|        key|        value|\n",
      "+-----------+-------------+\n",
      "|destination|United States|\n",
      "|     origin|      Germany|\n",
      "|      count|           10|\n",
      "|      count|           25|\n",
      "|     origin|       France|\n",
      "|destination|        Spain|\n",
      "|      count|           75|\n",
      "|destination|        Italy|\n",
      "|     origin|        Spain|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF2.select(explode(\"data_map\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, yes and no. Yes, `explode()` accepts both arrays as well as maps as an argument. No, because now I've lost the information, which three rows belong together. Additionally my intention was to gain three columns, one for each key value, and not just two. For maps referencing by key is always a better approach than referencing by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+\n",
      "|  destination| origin|count|\n",
      "+-------------+-------+-----+\n",
      "|United States|Germany|   10|\n",
      "|        Spain| France|   25|\n",
      "|        Italy|  Spain|   75|\n",
      "+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF2.select(\n",
    "    col(\"data_map\")[\"destination\"].alias(\"destination\"),\n",
    "    col(\"data_map\")[\"origin\"].alias(\"origin\"),\n",
    "    col(\"data_map\")[\"count\"].alias(\"count\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with Spark handling nearly unstructured data records of key:value pairs in different orders is not a big problem.\n",
    "\n",
    "Pyspark module <a href=https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions>pyspark.sql.functions</a> provides further map related functions, which I also just list here for later reference:\n",
    "\n",
    "* **map_concat()** - Returns the union of all the given maps\n",
    "* **map_from_entries()** - Collection function: Returns a map created from the given array of entries\n",
    "\n",
    "### Turning Arrays or Maps into JSON\n",
    "A nice Spark feature is the `to_json()` function which converts StructType, ArrayType or MapType data into JSON. This can be relevant for me if I have to call a REST API which expects JSON documents as paylod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|structstojson(data_map)                                        |\n",
      "+---------------------------------------------------------------+\n",
      "|{\"destination\":\"United States\",\"origin\":\"Germany\",\"count\":\"10\"}|\n",
      "|{\"count\":\"25\",\"origin\":\"France\",\"destination\":\"Spain\"}         |\n",
      "|{\"count\":\"75\",\"destination\":\"Italy\",\"origin\":\"Spain\"}          |\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "mapDF2.select(to_json(\"data_map\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- structstojson(data_map): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF2.select(to_json(\"data_map\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Semi-structured JSON data\n",
    "As I've learned on day 3, reading data from JSON file and transforming it into a DataFreame is quite simple. Just for repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .format(\"json\")\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.json\")\\\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what have I to to in case of having tabular data where only one column contains JSON strings? To check this out first I create same testdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "        [\n",
    "            (123, \"DUS\", '{\"destinations\" : [\"FRA\", \"MUC\", \"TXL\"], \"airlines\" : [\"LH\", \"EW\", \"RY\"]}'), \n",
    "            (456, \"FRA\", '{\"destinations\" : [\"CDG\", \"MUC\", \"JFK\"], \"airlines\" : [\"AF\", \"LH\", \"DL\"]}'),\n",
    "            (789, \"MUC\", '{\"destinations\" : [\"FRA\", \"ZUC\", \"DUS\"], \"airlines\" : [\"EW\", \"LH\", \"EW\"]}')\n",
    "        ], \n",
    "        [\"key\", \"airport\", \"dest\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------------------------------------------------------------+\n",
      "|key|airport|dest                                                                     |\n",
      "+---+-------+-------------------------------------------------------------------------+\n",
      "|123|DUS    |{\"destinations\" : [\"FRA\", \"MUC\", \"TXL\"], \"airlines\" : [\"LH\", \"EW\", \"RY\"]}|\n",
      "|456|FRA    |{\"destinations\" : [\"CDG\", \"MUC\", \"JFK\"], \"airlines\" : [\"AF\", \"LH\", \"DL\"]}|\n",
      "|789|MUC    |{\"destinations\" : [\"FRA\", \"ZUC\", \"DUS\"], \"airlines\" : [\"EW\", \"LH\", \"EW\"]}|\n",
      "+---+-------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation along JSON Paths\n",
    "Each row in the \"dest\" column contains a valid JSON document. Now I can use the `get_json_object()` function to access the values inside of the JSON documents by specifiying the path from the root element (represented by `$`) down the nesting hierarchie to the specific JSON obect I want to extract. \n",
    "\n",
    "path: `$.key_level1.key_level_2....key_level_n`\n",
    "\n",
    "Since in my DataFrame the objects \"destinations\", and \"airlines\" have value lists, I have to specify the list index to get one singular value per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-------+\n",
      "|key|airport|destination|airline|\n",
      "+---+-------+-----------+-------+\n",
      "|123|DUS    |TXL        |EW     |\n",
      "|456|FRA    |JFK        |LH     |\n",
      "|789|MUC    |DUS        |LH     |\n",
      "+---+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df.select(\n",
    "        \"key\", \n",
    "        \"airport\",\n",
    "        get_json_object(\"dest\", '$.destinations[2]').alias(\"destination\"),\n",
    "        get_json_object(\"dest\", '$.airlines[1]').alias(\"airline\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I omitt the list index, I'll get the entire value list in my result DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+----------------+\n",
      "|key|airport|destination        |airline         |\n",
      "+---+-------+-------------------+----------------+\n",
      "|123|DUS    |[\"FRA\",\"MUC\",\"TXL\"]|[\"LH\",\"EW\",\"RY\"]|\n",
      "|456|FRA    |[\"CDG\",\"MUC\",\"JFK\"]|[\"AF\",\"LH\",\"DL\"]|\n",
      "|789|MUC    |[\"FRA\",\"ZUC\",\"DUS\"]|[\"EW\",\"LH\",\"EW\"]|\n",
      "+---+-------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "        \"key\", \n",
    "        \"airport\",\n",
    "        get_json_object(\"dest\", '$.destinations').alias(\"destination\"),\n",
    "        get_json_object(\"dest\", '$.airlines').alias(\"airline\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a similar function `json_tuple()` but I'm not sure if it provides any benefits to me, because:\n",
    "1. I cannot use it if the JSON document has more than one level of nesting, and\n",
    "1. I cannot refer to single list elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+----------------+\n",
      "|key|airport|destination        |airline         |\n",
      "+---+-------+-------------------+----------------+\n",
      "|123|DUS    |[\"FRA\",\"MUC\",\"TXL\"]|[\"LH\",\"EW\",\"RY\"]|\n",
      "|456|FRA    |[\"CDG\",\"MUC\",\"JFK\"]|[\"AF\",\"LH\",\"DL\"]|\n",
      "|789|MUC    |[\"FRA\",\"ZUC\",\"DUS\"]|[\"EW\",\"LH\",\"EW\"]|\n",
      "+---+-------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "df.select(\"key\", \n",
    "          \"airport\",\n",
    "          json_tuple(\"dest\", \"destinations\", \"airlines\").alias(\"destination\", \"airline\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning JSON to Map based on Schema\n",
    "Finally, like I can read from JSON files using an explicit schema definition, I can also apply `from_json()` on DataFrame columns containing JSON by using a schema. Depending on the schema definition `from_json()` will return StructType, ArrayType or MapType. Actually I could perform a conversion round-trip  from StructType, ArrayType or MapType -> `to_json()` -> {Json} -> `from_json()` ->  StructType, ArrayType or MapType.\n",
    "\n",
    "I convert the Json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------------------------------------------------------+\n",
      "|key|airport|json_data                                                  |\n",
      "+---+-------+-----------------------------------------------------------+\n",
      "|123|DUS    |[destinations -> [FRA, MUC, TXL], airlines -> [LH, EW, RY]]|\n",
      "|456|FRA    |[destinations -> [CDG, MUC, JFK], airlines -> [AF, LH, DL]]|\n",
      "|789|MUC    |[destinations -> [FRA, ZUC, DUS], airlines -> [EW, LH, EW]]|\n",
      "+---+-------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "jsonSchema = MapType(\n",
    "    StringType(), \n",
    "    ArrayType(StringType(), True),\n",
    "    True\n",
    ")\n",
    "\n",
    "mappedDF = df.select(\"key\", \n",
    "          \"airport\",\n",
    "         from_json(\"dest\", jsonSchema).alias(\"json_data\")\n",
    ")\n",
    "\n",
    "mappedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can navigate on the Map structure to extract single values similar to navigating the JSON path using get_`json_object()`, e.g. grabbing the third element of the destinations lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------------+\n",
      "|key|airport|json_data[destinations][2]|\n",
      "+---+-------+--------------------------+\n",
      "|123|    DUS|                       TXL|\n",
      "|456|    FRA|                       JFK|\n",
      "|789|    MUC|                       DUS|\n",
      "+---+-------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mappedDF.select(\n",
    "    \"key\", \n",
    "    \"airport\",\n",
    "    col(\"json_data\")[\"destinations\"][2]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is: what is the benefit of taking these extra effort, defining a schema and converting JSON to Map? In my opinion this leads to cleaner code and a better design, because:\n",
    "1. now the JSON structure, a mexpecting is explicitly documented in the code by the schema instead of implicitly assumed \n",
    "1. the Map structure is a unifying abstraction of any key:value data, regardles of the source format, e.g. CSV file, JSON documents or key-value database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
